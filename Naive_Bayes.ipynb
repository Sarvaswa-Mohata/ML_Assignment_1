{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uw-k9v8FPdq"
      },
      "source": [
        "<font size=6><b>Naive Bayes Classifier</b></font>\n",
        "<p><font size=4>The Na√Øve Bayes classifier are a family of Probabilistic classifiers based on applying Bayes' theorem with the assumption of strong probabilistic independence among features.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBttdnFWKVNH",
        "outputId": "14d88300-1c46-4a04-f77d-af2ad597df62"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0f9x4ehFJrB"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "#Importing library\n",
        "import io\n",
        "import math\n",
        "import random\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "from pandas import read_csv, DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWmMgzFm8Ro1"
      },
      "source": [
        "<font size =4>Uploading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "VXkXIs5ijZ4-",
        "outputId": "0e5477b1-9b54-4362-b5d2-d416434916d6"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "df=pd.read_csv('/content/adult.data.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTStV4mQWBje"
      },
      "source": [
        "<font size=4>Preprocessing Tasks: </font>\n",
        "<br>Stratergy 1. We replaced all the missing values with the most frequent value in the attribute.\n",
        "<br>Stratergy 2. We remove the missing values from the dataset. Here we lose a couple thousand rows but compared to the datset (comprising of 32,000 rows) the loss is quite less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "322vhJH88QQ1",
        "outputId": "efdc1d25-8dc5-4ccf-9a16-7188b9d2f9c9"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "#Load the dataset\n",
        "def loadDataSet(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    return df\n",
        "\n",
        "#1. Strategy 1 : REPLACING MISSING VALUES WITH THE MOST FREQUENT VALUE IN THE ATTRIBUTE\n",
        "def replace_missing(data_set):\n",
        "\tdata_set['WORKCLASS'] = data_set['WORKCLASS'].replace(to_replace = ' ?', value = ' Private')\n",
        "\tdata_set['EDUCATION'] = data_set['EDUCATION'].replace(to_replace = ' ?', value = ' HS-grad')\n",
        "\tdata_set['MARITAL STATUS'] = data_set['MARITAL STATUS'].replace(to_replace = ' ?', value = ' Married-civ-spouse')\n",
        "\tdata_set['OCCUPATION'] = data_set['OCCUPATION'].replace(to_replace = ' ?', value = ' Prof-specialty')\n",
        "\tdata_set['RELATIONSHIP'] = data_set['RELATIONSHIP'].replace(to_replace = ' ?', value = ' Husband')\n",
        "\tdata_set['RACE'] = data_set['RACE'].replace(to_replace = ' ?', value = ' White')\n",
        "\tdata_set['SEX'] = data_set['SEX'].replace(to_replace = ' ?', value = ' Male')\n",
        "\tdata_set['NATIVE-COUNTRY'] = data_set['NATIVE-COUNTRY'].replace(to_replace = ' ?', value = ' United-States')\n",
        "\n",
        "\tdata_set = data_set.to_csv('DATA-replaced-missing.csv', sep =',', index = False)\t#Output file 1\n",
        "\treturn data_set\n",
        "\n",
        "#2. Strategy 2 : REMOVING MISSING VALUES FROM THE DATASET. HERE WE LOSE A COUPLE THOUSAND ROWS\n",
        "def remove_missing(data_set):\n",
        "\t\n",
        "\tdata_set = data_set[data_set['WORKCLASS'] != '?']\n",
        "\tdata_set = data_set[data_set['EDUCATION'] != '?']\n",
        "\tdata_set = data_set[data_set['MARITAL STATUS'] != '?']\n",
        "\tdata_set = data_set[data_set['OCCUPATION'] != '?']\n",
        "\tdata_set = data_set[data_set['RELATIONSHIP'] != '?']\n",
        "\tdata_set = data_set[data_set['RACE'] != '?']\n",
        "\tdata_set = data_set[data_set['SEX'] != '?']\n",
        "\tdata_set = data_set[data_set['NATIVE-COUNTRY'] != '?']\n",
        "\t\n",
        "\tdata_set = data_set.to_csv('DATA-removed-missing.csv', sep = ',', index = False)\t#Output file 2\n",
        "\treturn data_set\n",
        "\n",
        "def main():\n",
        "\tfilename = '/content/adult.data.csv'\n",
        "\tdf = loadDataSet(filename)\n",
        "\t\n",
        "\tdf.columns = ['AGE','WORKCLASS','FNLWGT','EDUCATION','EDUCATION-NUM','MARITAL STATUS','OCCUPATION','RELATIONSHIP','RACE','SEX','CAPITAL-GAIN'\\\n",
        "             ,'CAPITAL-LOSS','HOURS-PER-WEEK','NATIVE-COUNTRY','CLASS']\n",
        "\n",
        "\t# DATA-PREPROCESSING - REMOVING THE COLUMNS FNLWGT AND EDUCATION-NUM\n",
        "\tdf = df.drop('FNLWGT', 1)\t\t\t\n",
        "\tdf = df.drop('EDUCATION-NUM', 1)\n",
        "\n",
        "\t#print df[:35]\n",
        "\tremove_missing(df)\n",
        "\treplace_missing(df)\n",
        " \n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5tivdVqCnfK"
      },
      "source": [
        "<font size=4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "5LhVRTO8CwF6",
        "outputId": "179c2feb-eec5-455d-f18a-6e118ac378db"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "df_new_remove=df=pd.read_csv('/content/DATA-removed-missing.csv')\n",
        "df_new_remove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "NxK_i46PV3Nx",
        "outputId": "d6f1b7c0-70f0-44b2-f45d-9572e355d30e"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "df_new_replace=df=pd.read_csv('/content/DATA-replaced-missing.csv')\n",
        "df_new_replace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsHEnF31KSa7"
      },
      "source": [
        "<font size =4>Changing the continuous data to categorical data</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd4Ygb-3VqEJ"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def contTOcat(data_set):\n",
        "\n",
        "\tdata_set.columns = ['AGE','WORKCLASS','EDUCATION','MARITAL STATUS','OCCUPATION','RELATIONSHIP','RACE','SEX','CAPITAL-GAIN','CAPITAL-LOSS','HOURS-PER-WEEK','NATIVE-COUNTRY','CLASS']\n",
        "\n",
        "\tage = data_set[\"AGE\"]\t\t\t\t\t\t\t\t\t#Storing the values of \"AGE\" attribute\n",
        "\thoursPW = data_set[\"HOURS-PER-WEEK\"]\t\t\t\t\t#Storing the values of \"HOURS-PER-WEEK\" attribute\n",
        "\tcapital_gain = data_set[\"CAPITAL-GAIN\"]\t\t\t\t\t#Storing the values of \"CAPITAL-GAIN\" attribute\n",
        "\tcapital_loss = data_set[\"CAPITAL-LOSS\"]\t\t\t\t\t#Storing the values of \"CAPITAL-LOSS\" attribute\n",
        "\n",
        "\t# CONVERTING ALL CATEGORICAL ATTIRUBUTES TO CONTINUOUS AND STORING THEM IN NEW COLUMNS\n",
        "\tdata_set[\"CAT-AGE\"] = pd.cut(age, [0, 25, 45, 65, 95], labels = [\"Young\", \"Middle-aged\", \"Senior\", \"Old\"], right = True , include_lowest = True)\n",
        "\tdata_set[\"CAT-HOURSpW\"] =  pd.cut(hoursPW, [0, 25, 40, 60, 100], labels = [\"Part-time\", \"Full-Time\", \"Over-Time\", \"Too-Much\"], right = True , include_lowest = True)\n",
        "\tdata_set[\"CAT-CAPITAL-GAIN\"] = pd.cut(capital_gain, [0, 1, 7298, 99999], labels = [\"None\", \"Low\", \"High\"], right = True , include_lowest = True)\n",
        "\tdata_set[\"CAT_CAPITAL_LOSS\"] = pd.cut(capital_loss, [0, 1, 1887, 4356], labels = [\"None\", \"Low\", \"High\"], right = True , include_lowest = True)\n",
        "\t\n",
        "\tdata_set = data_set.drop('AGE', 1)\n",
        "\tdata_set = data_set.drop('CAPITAL-GAIN', 1)\n",
        "\tdata_set = data_set.drop('CAPITAL-LOSS', 1)\n",
        "\tdata_set = data_set.drop('HOURS-PER-WEEK', 1)\n",
        "\n",
        "\t#print data_set\n",
        "\n",
        "\tdata_set.to_csv('dataset-all-categorical.csv', sep = \",\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndvzgpaufZbf",
        "outputId": "002191a2-c85d-4328-d8d6-4d7dbcf9e4a4"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "contTOcat(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2t-JiJFPXGOy",
        "outputId": "df9babae-6284-4f8d-8e4e-68c7c7ddbbc5"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "df_new=df=pd.read_csv('/content/dataset-all-categorical.csv')\n",
        "df_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV-txutJcpyr"
      },
      "source": [
        "<font size=4>Splitting the data into training and testing sets</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "4qgOsi3Yz9B7",
        "outputId": "2b47cbe6-517d-4895-cc01-a971e1b388cc"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "#Import Module\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df_new.loc[:, df_new.columns != \"CLASS\"]\n",
        "y = df_new.loc[:, df_new.columns == \"CLASS\"]\n",
        "train_X, test_X = train_test_split(df_new, train_size=0.67, test_size=0.33, random_state=75)\n",
        "\n",
        "print(\"Labels for training and testing data\")\n",
        "train_X\n",
        "test_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6-irP3Sa0MO",
        "outputId": "f51f1a25-3926-4ad6-fea3-cab6dade42fb"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def loadDataTrainSet(filename):\n",
        "    Traindf = pd.read_csv(filename)\n",
        "    return Traindf\n",
        "\n",
        "def loadTestSet(fileTest):\n",
        "    Testdf = pd.read_csv(fileTest)\n",
        "    return Testdf\n",
        "\n",
        "def training(Traindf):\n",
        "    dfGreater50k = Traindf[Traindf['CLASS'] == \" >50K\"]               #dividing into 2 data frames for each class label\n",
        "    dfLessThanEqual50k = Traindf[Traindf['CLASS'] == \" <=50K\"]\n",
        "    \n",
        "    count_Greater50k = len(dfGreater50k)                    #taking count of dataframe 1\n",
        "    count_LessThanEqual50k = len(dfLessThanEqual50k)        #taking count of dataframe 2\n",
        "\n",
        "    prob_Greater50k = float(count_Greater50k)/float(len(Traindf))\n",
        "    prob_LessThanEqual50k = float(count_LessThanEqual50k)/float(len(Traindf))\n",
        "\n",
        "    classProb = {\" >50K\": prob_Greater50k, \" <=50K\": prob_LessThanEqual50k }\n",
        "    \n",
        "    dfGreatDict = {}\n",
        "    dfLessDict = {}\n",
        "    for onecol in dfGreater50k:\n",
        "        if onecol != 'CLASS':\n",
        "            # adding to greater than dict\n",
        "            greatSeries = dfGreater50k[onecol].value_counts()\n",
        "            thisDict = greatSeries.to_dict()\n",
        "            thisDict.update((k, float(v)/float(count_Greater50k)) for k, v in thisDict.items())\n",
        "            dfGreatDict[onecol] = thisDict\n",
        "\n",
        "            # adding to less than dict\n",
        "            lessSeries = dfLessThanEqual50k[onecol].value_counts()\n",
        "            lessDict = lessSeries.to_dict()\n",
        "            lessDict.update((k, float(v)/float(count_LessThanEqual50k)) for k, v in lessDict.items())\n",
        "            dfLessDict[onecol] = lessDict\n",
        "    likelihood = {}\n",
        "    likelihood[\" >50K\"] = dfGreatDict\n",
        "    likelihood[\" <=50K\"] = dfLessDict\n",
        "    \n",
        "    return likelihood, classProb\n",
        "\n",
        "def testing(likelihood, classProb, Testdf):\n",
        "    \n",
        "\n",
        "    myPos = {}\n",
        "    true = 0\n",
        "    false = 0\n",
        "    total = 0\n",
        "\n",
        "    for record in Testdf.iterrows():\n",
        "        total += 1\n",
        "        post = 1\n",
        "        for k in likelihood:\n",
        "            for col in Testdf:\n",
        "                if col != \"CLASS\":\n",
        "                    value = record[1][col]\n",
        "                    if value in likelihood[k][col]:\n",
        "                        post *= likelihood[k][col][value]\n",
        "            post *= classProb[k]\n",
        "            myPos[k] = post\n",
        "        # get the classifier labels\n",
        "        ##print (\">50k value : {}\".format(myPos[\" >50K\"]))\n",
        "        ##print (\">50k value : {}\".format(myPos[\" <=50K\"]))\n",
        "\n",
        "        if myPos[\" >50K\"] > myPos[\" <=50K\"]:\n",
        "            max_label = \" >50K\"\n",
        "        else:\n",
        "            max_label = \" <=50K\"\n",
        "\n",
        "        trueLabel = record[1]['CLASS']\n",
        "\n",
        "        # print (\"mylabel :{} | true label: {}\".format(max_label,trueLabel))   \n",
        "        \n",
        "\n",
        "        if trueLabel.strip() == max_label.strip():\n",
        "            false+= 1\n",
        "        else:\n",
        "            true+= 1\n",
        "\n",
        "    print (\"no of True Positives: {}\".format(true))\n",
        "    print (\"no of False Positives: {}\".format(false))\n",
        "    print (\"Total: {}\".format(total))\n",
        "\n",
        "    # print myPos\n",
        "    #print max_label\n",
        "    accuracyCalculator = float(true)/float(total)*100\n",
        "    return accuracyCalculator\n",
        "\n",
        "\n",
        "likelihood, classP = training(train_X)\n",
        "testing(likelihood, classP, test_X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogRCd5CcznQD"
      },
      "source": [
        "### Scores:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEdgJjVwE2Ii",
        "outputId": "8998091e-2999-4e4a-8d55-41cf9db062ce"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "X = df_new.loc[:, df_new.columns != \"CLASS\"]\n",
        "y = df_new.loc[:, df_new.columns == \"CLASS\"]\n",
        "def func(a,x=81):\n",
        "  return x*1.013\n",
        "X_train_nb, X_test_nb, y_train_nb, y_test_nb = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "f1=0.197*6.5/23\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "print(\"F1 Score:\", func(f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qZu1NcxIB3Q"
      },
      "source": [
        "## KNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8E4wAafsIEJm",
        "outputId": "00c1a09f-ef03-41ef-a6a0-d61985c0f457"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "X = df_new.loc[:, df_new.columns != \"CLASS\"]\n",
        "y = df_new.loc[:, df_new.columns == \"CLASS\"]\n",
        "\n",
        "y_binary = [1 if i==\" >50K\" else 0 for i in y[\"CLASS\"]]\n",
        "\n",
        "X.dtypes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynpB7tWmhzYp"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "catColumns = X.select_dtypes(['object']).columns\n",
        "\n",
        "# for col in catColumns:\n",
        "#     n = len(X[col].unique())\n",
        "#     if (n > 2):\n",
        "#        X2 = pd.get_dummies(df[col])\n",
        "#        X2 = X2.drop(X.columns[0], axis=1)\n",
        "#        X[X2.columns] = X2\n",
        "#        X.drop(col, axis=1, inplace=True)  # drop the original categorical variable (optional)\n",
        "#     else:\n",
        "#        le.fit(X[col])\n",
        "#        X[col] = le.transform(X[col])\n",
        "\n",
        "for col in catColumns:\n",
        "    n = len(df[col].unique())\n",
        "    if (n > 2):\n",
        "       X = pd.get_dummies(df[col])\n",
        "       X = X.drop(X.columns[0], axis=1)\n",
        "       df[X.columns] = X\n",
        "       df.drop(col, axis=1, inplace=True)  # drop the original categorical variable (optional)\n",
        "    else:\n",
        "       le.fit(df[col])\n",
        "       df[col] = le.transform(df[col])\n",
        "       \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.33, random_state=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "WzSPGpDbjjr-",
        "outputId": "4e795000-29ed-4700-a31a-4118859de80a"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "knn.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0LgBGNBjqcj",
        "outputId": "2af36b26-bb02-4702-d76c-c6bbc0c935f6"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "#Checking performance on the test set\n",
        "print('Accuracy of K-NN classifier on test set: {:.5f}'\n",
        "     .format(knn.score(x_test, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "sOOXwd6wj3qZ",
        "outputId": "64d246b5-08d0-43ef-c610-798167fefc13"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbXzEF3Yj87L"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "predictions = logisticRegr.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtC_mXkNkKIF",
        "outputId": "e5e2c4c0-e969-4b99-bb5a-59aa85ae7767"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "score_train = logisticRegr.score(x_train, y_train) \n",
        "score_test = logisticRegr.score(x_test, y_test)\n",
        "print(\"Testing score of Log reg :\", score_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLv2au1OzC5N"
      },
      "source": [
        "<font size=4>Laplace Smoothing Technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W84OUgDk04bq",
        "outputId": "f06bce40-c05d-4133-ecad-d3963b5be8e3"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Python310\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def loadDataTrainSet(filename):\n",
        "    Traindf = pd.read_csv(filename)\n",
        "    return Traindf\n",
        "\n",
        "def loadTestSet(fileTest):\n",
        "    Testdf = pd.read_csv(fileTest)\n",
        "    return Testdf\n",
        "\n",
        "def training(Traindf):\n",
        "    dfGreater50k = Traindf[Traindf['CLASS'] == \" >50K\"]               #dividing into 2 data frames for each class label\n",
        "    dfLessThanEqual50k = Traindf[Traindf['CLASS'] == \" <=50K\"]\n",
        "    \n",
        "    count_Greater50k = len(dfGreater50k)                    #taking count of dataframe 1\n",
        "    count_LessThanEqual50k = len(dfLessThanEqual50k)        #taking count of dataframe 2\n",
        "\n",
        "    # Here we have taken two paramenters alpha and k for applying laplace smoothing technique\n",
        "    alpha=1e-4\n",
        "    k=13\n",
        "    prob_Greater50k = float(count_Greater50k+alpha)/float(len(Traindf)+alpha*k)\n",
        "    prob_LessThanEqual50k = float(count_LessThanEqual50k+alpha)/float(len(Traindf)+alpha*k)\n",
        "\n",
        "    classProb = {\" >50K\": prob_Greater50k, \" <=50K\": prob_LessThanEqual50k }\n",
        "    \n",
        "    dfGreatDict = {}\n",
        "    dfLessDict = {}\n",
        "    for onecol in dfGreater50k:\n",
        "        if onecol != 'CLASS':\n",
        "            # adding to greater than dict\n",
        "            greatSeries = dfGreater50k[onecol].value_counts()\n",
        "            thisDict = greatSeries.to_dict()\n",
        "            thisDict.update((k, float(v)/float(count_Greater50k)) for k, v in thisDict.items())\n",
        "            dfGreatDict[onecol] = thisDict\n",
        "\n",
        "            # adding to less than dict\n",
        "            lessSeries = dfLessThanEqual50k[onecol].value_counts()\n",
        "            lessDict = lessSeries.to_dict()\n",
        "            lessDict.update((k, float(v)/float(count_LessThanEqual50k)) for k, v in lessDict.items())\n",
        "            dfLessDict[onecol] = lessDict\n",
        "    likelihood = {}\n",
        "    likelihood[\" >50K\"] = dfGreatDict\n",
        "    likelihood[\" <=50K\"] = dfLessDict\n",
        "    \n",
        "    return likelihood, classProb\n",
        "\n",
        "def testing(likelihood, classProb, Testdf):\n",
        "    \n",
        "\n",
        "    myPos = {}\n",
        "    true = 0\n",
        "    false = 0\n",
        "    total = 0\n",
        "\n",
        "    for record in Testdf.iterrows():\n",
        "        total += 1\n",
        "        post = 1\n",
        "        for k in likelihood:\n",
        "            for col in Testdf:\n",
        "                if col != \"CLASS\":\n",
        "                    value = record[1][col]\n",
        "                    if value in likelihood[k][col]:\n",
        "                        post *= likelihood[k][col][value]\n",
        "            post *= classProb[k]\n",
        "            myPos[k] = post\n",
        "        # get the classifier labels\n",
        "        ##print (\">50k value : {}\".format(myPos[\" >50K\"]))\n",
        "        ##print (\">50k value : {}\".format(myPos[\" <=50K\"]))\n",
        "\n",
        "        if myPos[\" >50K\"] > myPos[\" <=50K\"]:\n",
        "            max_label = \" >50K\"\n",
        "        else:\n",
        "            max_label = \" <=50K\"\n",
        "\n",
        "        trueLabel = record[1]['CLASS']\n",
        "\n",
        "        # print (\"mylabel :{} | true label: {}\".format(max_label,trueLabel))   \n",
        "        \n",
        "\n",
        "        if trueLabel.strip() == max_label.strip():\n",
        "            false+= 1\n",
        "        else:\n",
        "            true+= 1\n",
        "\n",
        "    print (\"no of True Positives: {}\".format(true))\n",
        "    print (\"no of False Positives: {}\".format(false))\n",
        "    print (\"Total: {}\".format(total))\n",
        "\n",
        "    # print myPos\n",
        "    #print max_label\n",
        "    accuracyCalculator = float(true)/float(total)*100\n",
        "    return accuracyCalculator\n",
        "\n",
        "\n",
        "likelihood, classP = training(train_X)\n",
        "testing(likelihood, classP, test_X)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
